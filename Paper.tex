\documentclass[12pt,oneside]{amsart}
%\usepackage{tweaklist}
\usepackage{cancel}
\usepackage{xspace}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{subfig}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[a4paper,width=170mm,top=18mm,bottom=22mm,includeheadfoot]{geometry}
\usepackage{booktabs}
\usepackage{array}
\usepackage{verbatim}
\usepackage{caption}
\usepackage{natbib}
\usepackage{float}
\usepackage{pdflscape}
\usepackage{mathtools}
\usepackage{cuted}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{afterpage}
\usepackage{tikz}
\usepackage{fancyvrb}
\usepackage[bookmarks=true, unicode=true, pdftitle={Ethereum Yellow Paper: a formal specification of Ethereum, a programmable blockchain}, pdfauthor={Dr. Gavin Wood},pdfkeywords={Ethereum, Yellow Paper, blockchain, virtual machine, cryptography, decentralised, singleton, transaction, generalised},pdfborder={0 0 0.5 [1 3]}]{hyperref}
%,pagebackref=true
\usepackage{tabu} %requires array.

\def\param{\lambda}
\def\reals{\mathbb{R}}
\def\file{\mathcal{F}}
\def\filehash{\sigma_F}
\def\key{K}
\def\keyhash{\sigma_K}
\def\hash{sha3}
\def\alice{\mathcal{A}}
\def\bob{\mathcal{B}}
\def\ether{\xi}
\def\zksnark{\zeta}
\def\encrypt{\varepsilon}
\def\decrypt{\delta}
\def\crp{\textit{Content Release Problem} }
\def\olp{\textit{Ownership Ledger Problem} }
\def\nb{\textbf{NB:} }
\newcommand{\st}{\mbox{ s.t. }}
\newcommand{\wrt}{\mbox{ w.r.t. }}
\newcommand{\etal}{\mbox{ et al. }}
\newcommand{\lfi}{\mbox{ LF1 }}
\newcommand{\lfii}{\mbox{ LF2 }}
\newcommand{\lfiii}{\mbox{ LF3 }}

%This should be the last package before \input{Version.tex}
\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}
% "hyperref loads the url package internally. Use \PassOptionsToPackage{hyphens}{url}\usepackage{hyperref} to pass the option to the url package when it is loaded by hyperref. This avoids any package option clashes." Source: <https://tex.stackexchange.com/questions/3033/forcing-linebreaks-in-url/3034#comment44478_3034>.
% Note also this: "If the \PassOptionsToPackage{hyphens}{url} approach does not work, maybe it's "because you're trying to load the url package with a specific option, but it's being loaded by one of your packages before that with a different set of options. Try loading the url package earlier than the package that requires it. If it's loaded by the document class, try using \RequirePackage[hyphens]{url} before the document class." Source: <https://tex.stackexchange.com/questions/3033/forcing-linebreaks-in-url/3034#comment555944_3034>.
% For more information on using the hyperref package, refer to e.g. https://en.wikibooks.org/w/index.php?title=LaTeX/Hyperlinks&stable=0#Hyperlink_and_Hypertarget.

\makeatletter
\newcommand{\linkdest}[1]{\Hy@raisedlink{\hypertarget{#1}{}}}
\makeatother
\usepackage{seqsplit}

% For formatting
%\usepackage{underscore}
%\usepackage{lipsum} % to generate filler text for testing of document rendering
\usepackage[english]{babel}
\usepackage[autostyle]{csquotes}
\MakeOuterQuote{"}

\usepackage[final]{microtype} % https://tex.stackexchange.com/questions/75140/is-it-possible-to-make-latex-mark-overfull-boxes-in-the-output#comment382776_75142

\input{Version.tex}
% Default rendering options
\definecolor{pagecolor}{rgb}{1,0.98,0.9}
\def\YellowPaperVersionNumber{unknown revision}
\IfFileExists{Options.tex}{\input{Options.tex}}

\newcommand{\hcancel}[1]{%
    \tikz[baseline=(tocancel.base)]{
        \node[inner sep=0pt,outer sep=0pt] (tocancel) {#1};
        \draw[black] (tocancel.south west) -- (tocancel.north east);
    }%
}%

\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}
\newcommand*\eg{e.g.\@\xspace}
\newcommand*\Eg{e.g.\@\xspace}
\newcommand*\ie{i.e.\@\xspace}
%\renewcommand{\itemhook}{\setlength{\topsep}{0pt}  \setlength{\itemsep}{0pt}\setlength{\leftmargin}{15pt}}

% REPORT REQUIREMENTS
% -------------------
%   6-8 pages
%
% Introduction
%   Problem Statement
%   Motivation
% Results
%   Solution Architecture
%   Implementation Details with Fuctionalities
% Summary/Conclusion
% Code Documentation

\title{ML-Runner: A Scalable Cloud Service for Machine \& Distributed Learning} %\\ {\smaller \textbf{Project Report \YellowPaperVersionNumber}}}
\author{
    Barak Ben Noon (bb2762@columbia.edu) \\
    James Maxwell Stenger (jms2431@columbia.edu)
}
\begin{document}
\pagecolor{pagecolor}

\begin{abstract}
Machine Learning (ML) models are playing an ever increasing role in consumer and enterprise applications.
Since roughly 2012, when Alex Krizhevsky \etal won the ImageNet Large-Scale Visual Recognition Challenge with a deep convolutional neural network algorithm called SuperVision,
machine learning systems have improved exponentially.
Today, ML systems now rival human capabilities in a large variety of application domains.
Asynchronous Deep Reinforcement Learning models such as OpenAI's \href{https://github.com/openai/retro}{Retro} defeat professional Dota2 players;
ML models power smart-assistant software such as Apple's Siri, Amazon's Alexa, and Google Home;
and in general ML is used effectively in applications ranging from stock trading to social network analysis.

ML models often include a number of hyperparameters which must be tuned in order to effectively train the model.
This tuning process is generally performed by data scientists who must be careful to avoid over or underfitting the model so as to optimize its performance on data outside the training set.
This process of adjusting the hyperparameters involves training and evaluating the model with each ``configuration'' of the parameters.

We sought to design an online platform which would allow end users to programmatically specify a set of ``configurations'' for an ML model which they want to tune.
The platform subsequently trains and evaluates the model in each those of configurations in the cloud, and makes the results available to the end user.
\end{abstract}

\maketitle
\setlength{\columnsep}{20pt}
\begin{multicols}{2}

% -------------- INTRODUCTION --------------
% ------------------------------------------
\section{Introduction}\label{sec:introduction}
Machine learning models typically involve some number of hyperparameters.
To tune a model, it is necessary to train and evaluate it over some `configuration space' of these hyperparameters in order to determine their optimal setting.

We call the process of training and evaluating a model for a fixed configuration of that model's hyperparameters `running an experiment'.
Tuning the model therefore is the process of `running an experiment' for each configuration of hyperparameters one wishes to evaluate and determining which configuration yields the most desireable results for that model (\eg minimizing its loss function, or maximizing its accuracy).

\subsection{Problem Statement} \label{ch:problem_statement}
There are existing web solutions for viewing the performance of an experiment given some configuration of its hyperparameters, but these services leave it to the end user to actually train the model using their own compute resources.
This is a nontrivial task, since running a single experiment often involves considerable compute resources, and the end user needs to run experiments for each configuration he wishes to evaluate.

\subsection{Proposal} \label{ch:problem_statement}
We therefore sought to implement a service that automates the process of running experiments for each configuration using cloud computing resources.
The user defines the set of `valid configurations' which are all of those configurations of a model's hyperparameters he wishes to evaluate; the service subsequently runs experiments for all of those configurations in the cloud.

\subsection{Motivation} \label{ch:motivation}
Such a service would free the end user from having to concern himself with the logistical and computational issues associated with running a large number of experiments in order to tune his model.
Since the end user is often a data scientist for whom the logistics of \textit{how} the model is trained are outside the scope of his work, freeing him of such concerns allows the end user to focus exclusively on the actual design and tuning of the model.

\subsection{Related Work} \label{ch:previous}
\href{https://comet.ml/}{Comet.ml} and \href{https://neptune.ml/}{Nuptune.ml} are two web services which facilitate the running of machine learning experiments with a web frontend that allows for experiment tracking and hyperparameter optimization.
They leave the actual training and testing of the ML experiments to the end user, \ie the end user must configure his computing resources to train and test the model himself.
Our service seeks to simplify the actual running of these experiments, and could potentially integrate with these services in the future.

% -------------- RESULTS --------------
% -------------------------------------
\section{Results} \label{ch:results}
We call the $n$ configurable hyperparameters of an ML model, or `experiment', $\param_i$ for $i \in [n]\}$.
The set of possible configurations of the model is then given by the cross of these hyperparameters, \ie
\[
C = \param_1 \times \param_2 \times \cdots \times \param_n
\]

If the space of some parameters is large or infinite, \eg $\param_i \in \reals$, this of course implies the space of valid configurations for the model is also infinite.
However, for such hyperparameters, the end user can easily define some set of values which he wishes to test, \eg $\lambda_i = \{ 10^i | i \in range(1, 10)\}$.

The simplest way to obtain the set of configurations which the end user wants to train and evaluate his model on is therefore to have him quickly and programmatically define the space of valid settings for each of his model's hyperparameters.
We can then infer the configurations as the cross of these valid settings.

For instance, if a model consists of 2 hyperparameters, and the end user indicates:

\begin{eqnarray}
\param_1 \in [\text{`a'}, \text{`b'}] \\
\param_2 \in [1, 2]
\end{eqnarray}

We can then define the set of configurations on which to train and evaluate the model on as:

\[
C = \{(a, 1), (a, 2), (b, 1), (b, 2)\}
\]

This is the system we implemented in our service to allow the end user to define the configuration space for each of his experiments.

\subsection{Solution Workflow}
We call a user of the service Alice.

Alice connects to the frontend, logs in, and creates a new experiment with a project name $P$.
To create the experiment, she uploads some code $C$ for her model, from which the frontend parses and detects the model's hyperparameters $\param_i$.
Alice then programmatically specifies the set of valid options $O$ for each $\param_i$, \eg $\param_1 \in range(1, 5)$.

The frontend then sends $P$, $C$, $O$ and Alice's username $U$ to the lambda function \lfi using API Gateway.

In \lfi, the set of configurations is infered from $O$ in the manner just described (\ie as the cross of the valid options for each $\param$).
\lfi subsequently creates a docker container containing the code $C$ and environment variables containing the parameter settings for each configuration.
It uploads this container to S3 under a key $K$, records loggin information for this $P$, $C$, $O$, and $U$ in a DynamoDB table.
Finally \lfi pushes a message $M$ = ($P$, $K$, `New Version') to SQS containing the location of the container in S3 and the project id for the experiment.

To run her experiments, Alice goes to the `runs' page of the service, which sends the $P$ and $K$ associated with a particular run to the lambda function \lfiii.

\lfiii pushes a message $M$ = ($P$, $K$, `Run') to SQS specifying the details of the runs Alice has selected.

Periodically, another lambda function \lfii checks the SQS queue for unconsumed messages.
This lambda handles creating new ElasticBeanstalk instances for experiments and running them upon request.
For messages matching ($P$, $K$, 'New Version'), it creates a new ElasticBeanstalk Application Version for that $P$ using the docker container found in S3 under key $K$.
For messages matching ($P$, $K$, 'Run'), it runs the ElasticBeanstalk Application Version associated with that $P$ and $K$.

\subsection{Solution Architecture}
The general architecture for our application was very similar to the one used for the three class assignments throughout the semester,
\ie a web frontend written in React that connected to a variety of microservices through AWS API Gateway.

We hosted the frontend on an AWS EC2 instance running an nginx server, but of course it could alternatively be hosted on S3 in the fashion described in the class assignments.
Authentication for API gateway was handled using AWS IAM and Cognito, which we integrated with the frontend using the `react-cognito' node package.

API gateway routed to three lambda functions which used a variety of other AWS services:

\begin{itemize}
  \item Elastic Beanstalk -- Used to train and evaluate each model configuration as present in its respective docker container
  \item SQS -- Used to manage the runs and create new AWS EB versions for each experiment and configuration
Used to handle communication and task queueing between \lfi, \lfii, and \lfiii.
  \item S3 -- Saved the docker containers we generated for each configuration of each experiment (and optionally hosted the frontend)
  \item Cloudwatch -- Used to schedule lambda functions which periodically pulled runs from SQS and executed them on AWS EB
\end{itemize}

\subsection{Implementation Details} \label{ch:implementation}

\end{multicols}
\subsubsection{Frontend}
We implemented a React front end which we deployed on an Nginx server on an AWS EC2 instance.
Our front end made use of a number of frameworks including:

\begin{itemize}
  \item aws-sdk
  \item react
  \item react-cognito (for user management and authentication with AWS Cognito)
  \item react-redux
  \item react-router
  \item react-router-redux
\end{itemize}

We provided pages for logging in, registering, learning about the service, checking one's profile, viewing a dashboard, creating new experiments, and viewing and running experiments.

\subsubsection{Backend: LF1}
See \ref{code:lf1} for the full javascript code.

\subsubsection{Backend: LF2}
See \ref{code:lf2} for the full javascript code.

\subsubsection{Backend: LF3}
See \ref{code:lf3} for the full javascript code.

\begin{multicols}{2}
% -------------- FUTURE DIRECTIONS --------------
% -----------------------------------------------
\section{Future Directions} \label{ch:future}
In our application we used AWS Elastic Beanstalk to run the containerized experiments for each configuration, since this is what we were familiar with.
It may be preferable to use AWS Elastic Container Service instead, since we already package the code in docker containers, though due to time constraints we did not get a chance to investigate this option.

% -------------- CONCLUSION --------------
% ----------------------------------------
\section{Conclusion} \label{ch:conclusion}
We have presented in this paper an architecture and near-complete implementation of an online service for concurrently training and testing machine learning models with a variety of hyperparameter configurations.

\section{Acknowledgements}
This paper was produced as part of Sambit Sahu's Spring 2018
\textit{Cloud Computing and Big Data} Course at Columbia University (\url{http://www.cs.columbia.edu/~sambits/})

\section{Availability}
The source of this paper is maintained at \url{https://github.com/6998/ml-runner-paper/}.
All project code can be found under the 6998 Organization at \url{https://github.com/6998}.
See \ref{ch:code} for a walkthrough of the code.

\end{multicols}
\bibliographystyle{plainnat}
\bibliography{Biblio}

\appendix

% -------------- CODE DOCUMENTATION --------------
% ------------------------------------------------
\section{Code Documentation} \label{ch:code}
\subsection{LF1: create-experiments} \label{code:lf1}
The javascript code for LF1 is shown below; the full deployment package is available at \url{https://github.com/6998/LF1}.
The lambda code is contained there as `index.js'.

\begin{Verbatim}[fontsize=\tiny, frame=single]
const fs = require('fs');
const path = require('path');
const AWS = require('aws-sdk');
const async = require('async');
const dynamoose = require('dynamoose');
const randomstring = require("randomstring");

const s3 = new AWS.S3({apiVersion: '2006-03-01'});
const sqs = new AWS.SQS({apiVersion: '2012-11-05'});

const DIR = process.env.DIR

// docker template
let dockerfile = "FROM python:3.4-alpine\n" +
"ADD . /code\n" +
"WORKDIR /code\n" +
"EXPOSE 5000\n" +
"RUN pip install -r requirements.txt\n" +
"CMD [\"python\", \"app.py\"]\n"

let finalArr = []; // for the combinations

// zipping part
var file_system = require('fs');
var archiver = require('archiver');

/* Dynamo */
const Run = dynamoose.model('Run', {
  id: {
    type: String,
    hashKey: false,
  },
  options: Object,
  s3: Object,
  err: Object,
  user: String,
  projectName: String,
  projectId: String,
  mode: String
}, { update: false });

const saveRuns = (items, user, cb) => {
  const arr = Object.keys(items).map(function(key) {
    return items[key];
  });
  Run.batchPut(arr, function (err, p) {
    if (err) {
      console.log(err);
    } else {
    }
    return cb(err);
  })
}

const createArray = (item) => {
  if (item.rangeOrType === "range") {
    const tmp = [];
    const from = parseFloat(item.range.from);
    const to = parseFloat(item.range.to);
    const jumps = parseFloat(item.jumps);
    for (let value = from; value <= to; value += jumps) {
      tmp.push({
        value: value.toFixed(3),
        key: item.key
      });
    }
    return tmp;
  } else if (item.rangeOrType === "options") {
    const tmp = item.values.split(",");
    return tmp.map(el => {
      return {
        value: el,
        key: item.key
      }
    })
  } else {
    return [];
  }
};

const allPossibleCombos = (options) => {
  let optionsArr = [];
  for (let k in options) { // create an array for each options
    optionsArr.push(options[k]);
  }

  optionsArr = optionsArr.map(el=>{ // convert each option to a range
    return createArray(el)
  });

  optionsArr = optionsArr.sort((a,b)=>{ // sort by length bigger first
    return b.length-a.length
  });

  optionsArr[0].forEach(item=>{
    finalArr.push([item]);
  });

  optionsArr = optionsArr.slice(1); // remove the first line

  optionsArr.forEach((opArr,a)=>{ // for each array of option
    const newSets = [];
    opArr.forEach((el,b)=>{ // for each element in the array
      const tmp = finalArr.map(el=>{
        return el.map(el2=>Object.assign(el2, {}));
      }); // deep clone

      tmp.forEach((arr,c)=>{ // for each set in the final array
        arr.push(el);
      }); // end adding element for each sub-set

      newSets.push(tmp);
    }); // end each element in the the range

    finalArr = [];
    newSets.forEach((item,d)=>{
      finalArr = finalArr.concat(item)
    })
  });

  return finalArr;
};

const renderFile = (codeArr, options)=>{
  options.forEach(item => {
    const key = item.key.split("_");
    codeArr[key[0]][key[1]] = `RUNNER_${item.key}`
  });

  let finalFile = "";
  codeArr.forEach(line=>{
    line.forEach(word=>{
      finalFile += word.replace(/\&nbsp\;/g, "\t")
    });
    finalFile += "\n";
  });

  return {finalFile, options};
};

const createZipAndUpload = (fileObj, jobNum, cb)=>{
  const file = fileObj.finalFile;
  const options = fileObj.options;
  // inject the os and the:

  let split = file.split("\n");

  // find the last line with the import statment
  let lastImportLine = 0;
  split.forEach((el, line)=>{
    if(el.includes("import")) { lastImportLine = line; }
  });

  const envVarsArr = options.map(item=>{
    return `RUNNER_${item.key}=os.environ['RUNNER_${item.key}']`;
  });

  const envVarsArrForDocker = options.map(item=>{
    return `ENV RUNNER_${item.key}=os.environ['RUNNER_${item.key}']`;
  });

  // create the app.py
  const part1 = split.slice(0, lastImportLine+1).join("\n");
  const part2 = envVarsArr.join("\n");
  const part3 = split.slice(lastImportLine+1, split.length).join("\n");
  const finalFile = [].concat(part1).concat(part2).concat(part3).join("\n");
  dockerfile = dockerfile += envVarsArrForDocker.join("\n");

  //create the Dockerfile
  fs.writeFileSync(`${DIR}/Dockerfile`, dockerfile);
  fs.writeFileSync(`${DIR}/app.py`, finalFile);

  // save the file
  const rand = randomstring.generate(7);
  const fileName = `job_num_${jobNum}_rand_${rand}.zip`;
  const filePath = `${DIR}/${fileName}`
  const output = file_system.createWriteStream(filePath);
  const archive = archiver('zip');

  output.on('close', function () {
    console.log('archiver has been finalized and the output file descriptor has closed.');
  });

  archive.on('error', function(err){
    console.log("error saving file:", err)
    // throw err;
  });

  output.on('end', function() {
    console.log('Data has been drained');
  });

  output.on('finish', function() {
    console.log('finish');
    const uploadParams = {Bucket: "docker-6998", Key: '', Body: ''};
    const fileStream = fs.createReadStream(filePath);

    fileStream.on('error', function(err) {
      console.log('fileStream Error', err);
    });
    uploadParams.Body = fileStream;
    uploadParams.Key = path.basename(fileName);

    // call S3 to retrieve upload file to specified bucket
    s3.putObject(uploadParams, function (err, data) {
      if (err) {
        console.log("putObject Error", err);
      } if (data) {
      }
      cb(err, data, uploadParams.Key, fileName);
    });
  });

  archive.pipe(output);
  archive.file(`${DIR}/requirements.txt`, "")
  archive.file(`${DIR}/Dockerfile`, "")
  archive.file(`${DIR}/app.py`, "")
  archive.file(`${DIR}/docker-compose.yml`, "")
  archive.finalize();
};

const addToSQS = (user, projectId, projectName, key, cb)=>{
  const MessageAttributes = {
    User: {
      DataType: "String",
      StringValue: user
    },
    ProjectName: {
      DataType: "String",
      StringValue: projectName
    },
    ProjectId: {
      DataType: "String",
      StringValue: projectId
    },
    S3key: {
      DataType: "String",
      StringValue: key
    },
  };

  var params = {
    DelaySeconds: 1,
    MessageAttributes,
    MessageBody: "New Version",
    QueueUrl: "https://sqs.us-east-1.amazonaws.com/906385631751/ml-runner"
  };

  sqs.sendMessage(params, function(err, data) {
    if (err) {
      console.log("sendMessage with sqs:", err);
    } else {
    }
    cb(err)
  });
}

exports.handler = (event, context, callback) => {
  let options = event.options;
  let codeArr = event.codeArr;
  let user = event.user;
  let projectName = event.projectName;

  const projectId = randomstring.generate(7);
  const allCombos = allPossibleCombos(options);

  let i = 0;
  const re = {};

  async.eachSeries(allCombos, (op, cb) => {
    const file = renderFile(codeArr, op);
    createZipAndUpload(file, i++, (err, data, key, fileName) => {
      const id = `job_${i}`;
      re[id] = {
        file: file.finalFile,
        options: file.options,
        s3: data,
        err,
        id: fileName,
        user: user,
        projectName,
        projectId,
        mode: "init"
      };
      addToSQS(user, projectId, projectName, key, (err)=>{
        cb();
      })
    })
  }, (err1) => {
    saveRuns(re, user, (err) => {
      console.log("save err", err);
      return callback(null, {files: re});
    })
  });
};

exports.createArray = createArray;
exports.allPossibleCombos = allPossibleCombos;
\end{Verbatim}

\subsection{LF2: create-EB} \label{code:lf2}
The javascript code for LF2 is shown below; the full deployment package is available at \url{https://github.com/6998/LF2}.
The lambda code is contained there as `index.js'.

\begin{Verbatim}[fontsize=\tiny, frame=single]
const AWS = require('aws-sdk');
const sqs = new AWS.SQS({apiVersion: '2012-11-05'});
const async = require('async');
const fs = require('fs');
const path = require('path');
const s3 = new AWS.S3({apiVersion: '2006-03-01'});
const randomstring = require("randomstring");
const dynamoose = require('dynamoose');
const elasticbeanstalk = new AWS.ElasticBeanstalk();

exports.handler = (event, context, callback) => {
  var queueURL = "https://sqs.us-east-1.amazonaws.com/906385631751/ml-runner";

  var params = {
    AttributeNames: [
      "SentTimestamp"
    ],
    MaxNumberOfMessages: 10,
    MessageAttributeNames: [
      "All"
    ],
    QueueUrl: queueURL,
    VisibilityTimeout: 0,
    WaitTimeSeconds: 0
  };

  sqs.receiveMessage(params, function(err, data) {
    async.eachSeries(data.Messages, (item, cb)=>{
      console.log(tem.MessageAttributes)

      const projectName = item.MessageAttributes.ProjectName.StringValue;
      const ProjectId = item.MessageAttributes.ProjectId.StringValue;
      const S3key = item.MessageAttributes.S3key.StringValue;
      const name = `runner_${projectName}_${ProjectId}`;

      if(item.Body === "New Version") {
        const EBparams = {
          ApplicationName: name,
          AutoCreateApplication: true,
          Description: `runner_${projectName}`,
          Process: true,
          SourceBundle: {
            S3Bucket: "docker-6998",
            S3Key: S3key
          },
          VersionLabel: S3key
        };

        var deleteParams = {
          QueueUrl: queueURL,
          ReceiptHandle: item.ReceiptHandle
        };

        sqs.deleteMessage(deleteParams, function(err, data) {
            console.log("Delete Error", err);
        });

        // Create EB version
        elasticbeanstalk.createApplicationVersion(EBparams, function(err, data) {
          console.log("elasticbeanstalk ", err);
          cb();
        });
      } else if(item.Body === "Run") {
        const rand = randomstring.generate(3);
        let env = S3key.split["."];
        env = env[0];

        var params = {
          ApplicationName: name,
          CNAMEPrefix: projectName+rand,
          EnvironmentName: env,
          SolutionStackName: "64bit Amazon Linux 2017.09 v2.9.2 running Docker 17.12.0-ce",
          VersionLabel: S3key
        };

        // Launch EB instance
        elasticbeanstalk.createEnvironment(params, function(err, data) {
          console.log("createEnvironment: ", err);
          cb();
        });
      } else {
        cb();
      }

    }, ()=>{
      return callback(null, true);
    })
  });
};
\end{Verbatim}

\subsection{LF3: create-run} \label{code:lf3}
The javascript code for LF3 is shown below; the full deployment package is available at \url{https://github.com/6998/LF3}.
The lambda code is contained there as `index.js'.

\begin{Verbatim}[fontsize=\tiny, frame=single]
const AWS = require('aws-sdk');
const sqs = new AWS.SQS({apiVersion: '2012-11-05'});
const async = require('async');
const fs = require('fs');
const path = require('path');
const s3 = new AWS.S3({apiVersion: '2006-03-01'});
const randomstring = require("randomstring");
const dynamoose = require('dynamoose');
const elasticbeanstalk = new AWS.ElasticBeanstalk();

exports.handler = (event, context, callback) => {
  console.log( event)

  const MessageAttributes = {
    User: {
      DataType: "String",
      StringValue: event.user
    },
    ProjectName: {
      DataType: "String",
      StringValue: event.projectName
    },
    ProjectId: {
      DataType: "String",
      StringValue: event.projectId
    },
    S3key: {
      DataType: "String",
      StringValue: event.S3key
    },
  };

  var params = {
    DelaySeconds: 1,
    MessageAttributes,
    MessageBody: "Run",
    QueueUrl: "https://sqs.us-east-1.amazonaws.com/906385631751/ml-runner"
  };

  sqs.sendMessage(params, function(err, data) {
    if (err) {
      console.log("sendMessage with sqs:", err);
    } else {
      console.log("added new run to the queue")
    }
    return callback(null, true);
  });
};
\end{Verbatim}

% -------------- TERMINOLOGY --------------
% -----------------------------------------
\section{Terminology} \label{ch:Terminology}

\begin{description}
\item[address] triple abc
\end{description}

\appendix
\end{document}
